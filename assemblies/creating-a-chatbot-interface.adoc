:_module-type: ASSEMBLY

ifdef::context[:parent-context: {context}]

[id="creating-a-chatbot-interface_{context}"]
= Creating a chatbot interface

[role='_abstract']
As an OpenShift cluster administrator, you can configure and implement a chat bot interface that uses Retrieval-Augmented Generation (RAG). AI engineers can then use the chat bot interface in {productname-short}. To configure and implement a chat bot interface that uses RAG, complete the following tasks: 

* Install and configure the LlamaStack Operator.
* Enable GPU support on the OpenShift cluster. This task includes installing the required NVIDIA Operators.
* Configure vLLM as the inference provider.
* Deploy a Llama model by using vLLM. This task includes creating a storage connection and configuring GPU allocation.
* Configure a vector store, for example, Milvus, and ingest domain data for retrieval.
* Integrate LlamaStack with the inference service to enable RAG.
* Expose and secure the model endpoints.
* Configure a chat client to send queries to LlamaStack.
* Enable monitoring, logging, and autoscaling for inference and retrieval components.
* Validate end-to-end functionality with sample queries and optimize performance.

include::modules/installing-the-llamastack-operator.adoc[leveloffset=+1]
//include::modules/provisioning-nvidia-gpus-for-llama-stack-workloads.adoc[leveloffset=+1]

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]