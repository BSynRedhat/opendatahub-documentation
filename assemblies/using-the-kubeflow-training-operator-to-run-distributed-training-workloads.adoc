:_module-type: ASSEMBLY

ifdef::context[:parent-context: {context}]


[id="using-the-kubeflow-training-operator-to-run-distributed-training-workloads_{context}"]
= Using the Kubeflow Training Operator to run distributed training workloads 

[role='_abstract']
You can use the KFTO `PyTorchJob` API to configure the job to run on multiple nodes with multiple accelerators per node.

You configure the PyTorch training script, create a PyTorch job to run the training script, and then run the PyTorch training job on multiple nodes with multiple GPUs.

You can store the training script in a `ConfigMap` resource, or in a custom container image.


include::modules/configuring-a-kfto-pytorch-training-script-configmap-resource.adoc[leveloffset=+1]
include::modules/configuring-a-containerized-kfto-pytorch-training-script.adoc[leveloffset=+1]
include::modules/configuring-a-kfto-pytorchjob-resource.adoc[leveloffset=+1]
include::modules/configuring-a-kfto-pytorchjob-resource-by-using-the-cli.adoc[leveloffset=+1]
include::./example-kfto-pytorch-training-scripts.adoc[leveloffset=+1]
include::modules/ref-example-kfto-pytorchjob-resource-for-multi-node-training.adoc[leveloffset=+1]

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]
