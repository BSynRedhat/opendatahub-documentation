:_module-type: PROCEDURE

[id="building-a-custom-llama-stack-container-image_{context}"]
= Building a custom Llama Stack container image

[role='_abstract']
You can use a default Llama Stack container image for most deployments. However, if you require custom configuration, such as additional providers or model support, you can build your own image and then push it to a container registry, such as Quay.io.


.Prerequisites

* You have cluster administrator privileges for your {openshift-platform} cluster.
* You have logged in to {productname-long}.
* You have installed the Llama Stack Operator.
* You have push access to a container registry, such as Quay.io.
* You have deployed, or have access to, an external vLLM service endpoint.
* You have deployed, or have access to, an external Milvus vector database endpoint, including any required credentials.
* You have identified the values for any required environment variables in your LlamaStackDistribution custom resource, such as the inference model, vLLM endpoint URL, and Milvus database path.
* The project namespace where you will deploy the LlamaStackDistribution has sufficient resource quota, persistent storage available, and any required image pull secrets to access private container registries.
ifdef::upstream,self-managed[]
* You have installed the OpenShift command line interface (`oc`) as described in link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^].
endif::[]
ifdef::cloud-service[]
* You have installed the OpenShift command line interface (`oc`) as described in link:https://docs.redhat.com/en/documentation/openshift_dedicated/{osd-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI (OpenShift Dedicated)^] or link:https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/{rosa-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI (Red Hat OpenShift Service on AWS)^].
endif::[]
* You have installed Podman on your local machine. You can use Docker as an alternative.
* You have installed uv on your local machine. For more information, see https://docs.astral.sh/uv/[uv] in the uv documentation.

.Procedure

. Clone the `llama-stack` repository:
+
[source,terminal]
----
$ git clone https://github.com/meta-llama/llama-stack.git
$ cd llama-stack
----

. Use the `remote-vllm` template for your build, which includes the files `build.yaml` and `run.yaml` in the `llama_stack/templates/remote-vllm` directory.

. Update the `build.yaml` file to use Milvus as the only vector database and set the image type to `container`:
+
.File: `llama_stack/templates/remote-vllm/build.yaml`
[source,yaml]
----
version: '2'
distribution_spec:
  description: Use an external vLLM server for running LLM inference
  providers:
    inference:
      - remote::vllm
      - inline::sentence-transformers
    vector_io:
      - inline::milvus
  # ... other providers unchanged
  image_type: container
----

. Update the `run.yaml` file to specify your Milvus vector database, set the embedding model, and define the relevant providers and model references.
+
.File: `llama_stack/templates/remote-vllm/run.yaml`
[source,yaml]
----
providers:
  inference:
    - provider_id: vllm-inference
      provider_type: remote::vllm
      config:
        url: ${env.VLLM_URL:http://localhost:8000/v1}
        max_tokens: ${env.VLLM_MAX_TOKENS:4096}
        api_token: ${env.VLLM_API_TOKEN:fake}
        tls_verify: ${env.VLLM_TLS_VERIFY:true}
    - provider_id: sentence-transformers
      provider_type: inline::sentence-transformers
      config: {}
  vector_io:
    - provider_id: milvus
      provider_type: inline::milvus
      config:
        db_path: ${env.MILVUS_DB_PATH}
models:
  metadata: {}
  model_id: ${env.INFERENCE_MODEL}
  provider_id: vllm-inference
  model_type: llm
  metadata:
    embedding_dimension: 768
  model_id: ibm-granite/granite-embedding-125m-english
  provider_id: sentence-transformers
  model_type: embedding
# ... other sections unchanged
----

. Build the Llama Stack image:
+
[source,terminal]
----
$ CONTAINER_BINARY=podman BUILD_PLATFORM=linux/amd64 USE_COPY_NOT_MOUNT=true LLAMA_STACK_DIR=. uv run --with llama-stack llama stack build --template remote-vllm --image-type container
----
+
This commands creates a `remote-vllm` container image.

. Enter the following command to list all container images on your system and verify that a `remote-vllm` image has been created.
+
[source,terminal]
----
$ podman images
----
+
Here is some example output that displays the new container image:
+
[source,terminal]
----
REPOSITORY                            TAG        IMAGE ID       CREATED          SIZE
localhost/llama-stack-remote-vllm-dev latest     8d34e2a6eae1   3 minutes ago    4.2 GB
quay.io/myuser/llama-stack:mytag      mytag      97c88a1bbd23   5 minutes ago    4.2 GB
----

. Tag and push your image to your container registry. Replace placeholders with your image details:
+
[source,terminal]
----
$ podman tag <local-image> quay.io/<your-namespace>/<your-image>:<tag>
$ podman push <container registry path>/<your-namespace>/<your-image>:<tag>
----

[NOTE]
====
ifndef::upstream[]
Before you can deploy your custom image, you must create or update the `LlamaStackDistribution` custom resource (CR) in your project to specify your custom image and any required environment variables in the CR YAML, as described in link:{rhoaidocshome}{default-format-url}/working_with_rag/creating-a-chatbot-interface_rag#creating-a-llama-stack-distribution_rag[Creating a Llama Stack Distribution]. 
endif::[]

ifdef::upstream[]
Before you can deploy your custom image, you must create or update the `LlamaStackDistribution` custom resource (CR) in your project to specify your custom image and any required environment variables in the CR YAML, as described in link:{odhdocshome}/working-with-rag/creating-a-chatbot-interface/#creating-a-llama-stack-distribution_rag[Creating a Llama Stack Distribution]. 
endif::[]
====

.Verification

. Open a new terminal window.
. Log in to your {openshift-platform} cluster from the CLI:
. In the upper-right corner of the OpenShift web console, click your user name and select *Copy login command*.
.. After you have logged in, click *Display token*.
.. Copy the *Log in with this token* command and paste it in the OpenShift command-line interface (CLI).
+
[source,subs="+quotes"]
----
$ oc login --token=__<token>__ --server=__<openshift_cluster_url>__
----

. Confirm that your container image appears in your container registry, such as Quay.io:
+
[source,terminal]
----
$ podman search <container registry path>
----
+
Alternatively, use the container registry web interface to check the repository and confirm that the new tag is present.



