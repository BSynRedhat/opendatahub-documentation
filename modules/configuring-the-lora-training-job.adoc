:_module-type: PROCEDURE

[id="configuring-the-lora-training-job_{context}"]
= Configuring the LoRA training job

[role='_abstract']
Before you can use a LoRA training job to tune a model, you must configure the training job. 
The example training job in this section is based on the IBM and Hugging Face tuning example provided link:https://github.com/foundation-model-stack/fms-hf-tuning/tree/main/examples/prompt_tuning_twitter_complaints[here]. 


.Prerequisites
ifdef::upstream,self-managed[]
* You have logged in to {openshift-platform}.
endif::[]
ifdef::cloud-service[]
* You have logged in to OpenShift.
endif::[]

ifndef::upstream[]
* You have access to a data science cluster that is configured to run distributed workloads as described in link:{rhoaidocshome}{default-format-url}/working_with_distributed_workloads/configuring-distributed-workloads_distributed-workloads[Configuring distributed workloads].
endif::[]
ifdef::upstream[]
* You have access to a data science cluster that is configured to run distributed workloads as described in link:{odhdocshome}/working-with-distributed-workloads/#configuring-distributed-workloads_distributed-workloads[Configuring distributed workloads].
endif::[]

ifndef::upstream[]
* You have created a data science project. 
For information about how to create a project, see link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/using-data-science-projects_projects#creating-a-data-science-project_projects[Creating a data science project].
endif::[]
ifdef::upstream[]
* You have created a data science project. 
For information about how to create a project, see link:{odhdocshome}/working-on-data-science-projects/#creating-a-data-science-project_projects[Creating a data science project].
endif::[]

* You have Admin access for the data science project.
** If you created the project, you automatically have Admin access. 
** If you did not create the project, your cluster administrator must give you Admin access.

ifndef::upstream[]
* If Kueue is not installed in your environment, you have configured the Training Operator permissions as described in link:{rhoaidocshome}{default-format-url}/working_with_distributed_workloads/tuning-a-model-by-using-the-training-operator_distributed-workloads/#configuring-the-training-operator-permissions-when-not-using-kueue_distributed-workloads[Configuring the Training Operator permissions when not using Kueue].
endif::[]
ifdef::upstream[]
* If Kueue is not installed in your environment, you have configured the Training Operator permissions as described in link:{odhdocshome}/working-with-distributed-workloads/#configuring-the-training-operator-permissions-when-not-using-kueue_distributed-workloads[Configuring the Training Operator permissions when not using Kueue].
endif::[]

* You have access to a model.
* You have access to data that you can use to train the model.

.Procedure
. In a terminal window, if you are not already logged in to your OpenShift cluster, log in to the OpenShift CLI as shown in the following example:
+
[source,subs="+quotes"]
----
$ oc login __<openshift_cluster_url>__ -u __<username>__ -p __<password>__
----

. Configure a LoRA training job, as follows:
.. Create a YAML file named `lora-config.yaml`.
.. Add the following `ConfigMap` object definition:
+
[source]
----
kind: ConfigMap
apiVersion: v1
metadata:
  name: lora-config
  namespace: kfto
data:
  lora_config.json: |
    {
      "model_name_or_path": "bigscience/bloom-560m",
      "training_data_path": "/data/input/twitter_complaints.json",
      "output_dir": "/data/output/tuning/bloom-twitter",
      "num_train_epochs": 10.0,
      "per_device_train_batch_size": 4,
      "gradient_accumulation_steps": 4,
      "learning_rate": 1e-05,
      "response_template": "\n### Label:",
      "dataset_text_field": "output",
      "use_flash_attn": false,
      "lora_r": 8,
      "lora_alpha": 8,
      "lora_dropout": 0.1,
      "bias": "none",
      "target_modules": ["q_proj", "v_proj"]
    }

----
.. Edit the training job metadata as shown in the following table.
+
.Training job metadata
[cols="16,84"]
|===
|Parameter | Value

|`name`
|Name of the training job

|`namespace`
|Name of your project
|===

.. Edit the parameters of the training job as shown in the following table.
+
.Training job parameters
[cols="30,70"]
|===
|Parameter | Value

|`model_name_or_path`
|Name of the pre-trained model

|`training_data_path`
|Path to the training data that you set in the `training_data.yaml` config map

|`output_dir`
|Output directory for the model

|`num_train_epochs`
|Number of epochs for training; in this example, the training job is set to run 10 times

|`per_device_train_batch_size`
|Batch size, the number of data set examples to process together; in this example, the training job processes 4 examples at a time

|`gradient_accumulation_steps`
|Number of gradient accumulation steps

|`learning_rate`
|Learning rate for the training

|`response_template`
|Template formatting for the response

|`dataset_text_field`
|Dataset field for training output, as set in the `training_data.yaml` config map

|`use_flash_attn`
|Whether to use flash attention

|`lora_r`
|Rank of the low-rank decomposition

|`lora_alpha`
|Scale the low-rank matrices to control their influence on the model's adaptations

|`lora_dropout`
|Dropout rate applied to the LoRA layers, a regularization technique to prevent overfitting

|`bias`
|Whether to adapt bias terms in the model; setting the bias to “none” indicates that no bias terms will be adapted

|`target_modules`
|Names of the modules to apply LoRA to; includes ‘q_proj” and ‘v_proj’ by default

|===
.. Save your changes in the `lora-config.yaml` file.
.. Apply the configuration to create the `lora-config` object:
+
[source]
----
$ oc apply -f lora-config.yaml
----

. Create the training data, as follows:
.. Create a YAML file named `training_data.yaml`.
.. Add the following `ConfigMap` object definition:
+
[source]
----
kind: ConfigMap
apiVersion: v1
metadata:
  name: twitter-complaints
  namespace: kfto
data:
  twitter_complaints.json: |
    [
        {"Tweet text":"@HMRCcustomers No this is my first job","ID":0,"Label":2,"text_label":"no complaint","output":"### Text: @HMRCcustomers No this is my first job\n\n### Label: no complaint"},
        {"Tweet text":"@KristaMariePark Thank you for your interest! If you decide to cancel, you can call Customer Care at 1-800-NYTIMES.","ID":1,"Label":2,"text_label":"no complaint","output":"### Text: @KristaMariePark Thank you for your interest! If you decide to cancel, you can call Customer Care at 1-800-NYTIMES.\n\n### Label: no complaint"},
        {"Tweet text":"@EE On Rosneath Arial having good upload and download speeds but terrible latency 200ms. Why is this.","ID":3,"Label":1,"text_label":"complaint","output":"### Text: @EE On Rosneath Arial having good upload and download speeds but terrible latency 200ms. Why is this.\n\n### Label: complaint"},
        {"Tweet text":"Couples wallpaper, so cute. :) #BrothersAtHome","ID":4,"Label":2,"text_label":"no complaint","output":"### Text: Couples wallpaper, so cute. :) #BrothersAtHome\n\n### Label: no complaint"},
        {"Tweet text":"@mckelldogs This might just be me, but-- eyedrops? Artificial tears are so useful when you're sleep-deprived and sp… https:\/\/t.co\/WRtNsokblG","ID":5,"Label":2,"text_label":"no complaint","output":"### Text: @mckelldogs This might just be me, but-- eyedrops? Artificial tears are so useful when you're sleep-deprived and sp… https:\/\/t.co\/WRtNsokblG\n\n### Label: no complaint"},
        {"Tweet text":"@Yelp can we get the exact calculations for a business rating (for example if its 4 stars but actually 4.2) or do we use a 3rd party site?","ID":6,"Label":2,"text_label":"no complaint","output":"### Text: @Yelp can we get the exact calculations for a business rating (for example if its 4 stars but actually 4.2) or do we use a 3rd party site?\n\n### Label: no complaint"},
        {"Tweet text":"@nationalgridus I have no water and the bill is current and paid. Can you do something about this?","ID":7,"Label":1,"text_label":"complaint","output":"### Text: @nationalgridus I have no water and the bill is current and paid. Can you do something about this?\n\n### Label: complaint"},
        {"Tweet text":"@JenniferTilly Merry Christmas to as well. You get more stunning every year ��","ID":9,"Label":2,"text_label":"no complaint","output":"### Text: @JenniferTilly Merry Christmas to as well. You get more stunning every year ��\n\n### Label: no complaint"}
    ]

----
.. Replace the example namespace value `kfto` with the name of your project.
.. Replace the example training data with your training data.
.. Save your changes in the `training_data.yaml` file.
.. Apply the configuration to create the training data:
+
[source]
----
$ oc apply -f training_data.yaml
----

. Create a persistent volume claim (PVC), as follows:
.. Create a YAML file named `trainedmodelpvc.yaml`.
.. Add the following `PersistentVolumeClaim` object definition:
+
[source]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: trained-model
  namespace: kfto
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi

----
.. Replace the example namespace value `kfto` with the name of your project, and update the other parameters to suit your environment.
.. Save your changes in the `trainedmodelpvc.yaml` file.
.. Apply the configuration to create a Persistent Volume Claim (PVC) for the training job:
+
[source]
----
$ oc apply -f trainedmodelpvc.yaml
----





.Verification
ifdef::upstream,self-managed[]
. In the {openshift-platform} console, select your project from the *Project* list. 
endif::[]
ifdef::cloud-service[]
. In the OpenShift console, select your project from the *Project* list.
endif::[]
. Click *ConfigMaps* and verify that the `lora-config` and `twitter-complaints` config maps are listed. 
. Click *Search*, and from the *Resources* list, select *PersistentVolumeClaim*. Verify that the `trained-model` PVC is listed.


////
[role='_additional-resources']
.Additional resources
<Do we want to link to additional resources?>


* link:https://url[link text]
////
