:_module-type: PROCEDURE

[id="creating-a-llama-stack-distribution_{context}"]
= Creating a Llama Stack Distribution

[role='_abstract']
You can integrate LlamaStack and its retrieval-augmented generation (RAG) capabilities with your deployed Llama 3.2 model served by vLLM. This integration enables you to build intelligent applications that combine large language models (LLMs) with real-time data retrieval, providing more accurate and contextually relevant responses for your AI workloads.

.Prerequisites

* You have cluster administrator privileges for your {openshift-platform} cluster.
* You have logged in to {productname-long}.
* You have installed the Llama Stack Operator.
* You have successfully deployed a vLLM model server with a Llama 3.2 model and have the inference service URL.
* If you plan to use a vector database such as Milvus for your RAG workloads, you have configured the database and have the database connection path available.
* You have access to S3-compatible object storage, URI-based repository, or OCI-compliant registry.

.Procedure

. Log in to the OpenShift web console.
. From the left-hand navigation, select *Administrator* view.
. Click the *Quick Create* (image:images/quick-create-icon.png[]) icon and then click the *Import YAML* option.
. In the *YAML* editor that appears, create a custom resource definition (CRD) similar to the following example. Edit the `namespace` and other values as required for your environment:
+
[source,yaml]
----
apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: llama-custom-distribution
  namespace: llamastack
spec:
  replicas: 1
  server:
    containerSpec:
      env:
        - name: VLLM_URL
          value: 'http://llama-32-3b-instruct-predictor:80/v1'
        - name: INFERENCE_MODEL
          value: llama-32-3b-instruct
        - name: MILVUS_DB_PATH
          value: /.llama/distributions/remote-vllm/milvus.db
        - name: VLLM_TLS_VERIFY
          value: 'false'
      name: llama-stack
      port: 8321
    distribution:
      image: '<container image path>'
    podOverrides:
      volumeMounts:
        - mountPath: /root/.llama
          name: llama-storage
      volumes:
        - emptyDir: {}
          name: llama-storage
----
. Click *Create*.

.Verification

* In the left-hand navigation, click *Workloads* → *Pods* and then verify that the LlamaStack pod is running in the correct namespace.
* To verify that the LlamaStack server is running, click the pod name and select the *Logs* tab. Look for output similar to the following:
+
[source,log]
----
INFO     2025-05-15 11:23:52,750 __main__:498 server: Listening on ['::', '0.0.0.0']:8321
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO     2025-05-15 11:23:52,765 __main__:151 server: Starting up
INFO:     Application startup complete.
INFO:     Uvicorn running on http://['::', '0.0.0.0']:8321 (Press CTRL+C to quit)
----
* Confirm that a Service resource for the LlamaStack backend is present in your namespace and points to the running pod. You can check this by clicking *Networking* → *Services* in the web console.