:_module-type: PROCEDURE

[id="Customizing-model-selection-options_{context}"]
= Customizing model selection options for the NVIDIA NIM model serving platform

[role="_abstract"]
The NVIDIA NIM model serving platform includes all the NVIDIA NIM models available in the NVIDIA GPU Cloud (NGC) catalog for selection and deployment. You can select a NIM model of your choice from the *NVIDIA NIM* list in the *Deploy model* dialog. You can customize the models available for selection by creating a `ConfigMap` object that contains a list of preferred models.

.Prerequisites

* You have cluster administrator privileges for your {openshift-plaform} cluster.
* You have an NVIDIA Cloud Account (NCA) and can access the NVIDIA GPU Cloud (NGC) portal. 
* You know the ID or IDs of the models that you want to include for selection on the NVIDIA NIM model serving platform.
+
[NOTE]
====
* You can find the ID of a model from the link:https://catalog.ngc.nvidia.com/[NGC Catalog]. The ID is usually part of the URL path.
* You can also find the ID of a model by using the NGC CLI.  For more information, see link:https://docs.ngc.nvidia.com/cli/cmd_registry.html#model[NGC CLI reference].
====
* You know the name and namespace of your `Account` custom resource (CR).

.Procedure

. In a terminal window, log in to the {openshift-platform} CLI as a cluster administrator as shown in the following example:
+
[source, console]
----
oc login <openshift_cluster_url> -u <admin_username> -p <password>
----
. Define a `ConfigMap` object in a YAML file, similar to the one in the following example, containing the IDs of the models that you want to include for selection on the NVIDIA NIM model serving platform:
+
[source, yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
 name: nvidia-nim-enabled-models
data:
 models: |-
    [
    "mistral-nemo-12b-instruct",
    "llama3-70b-instruct",
    "phind-codellama-34b-v2-instruct",
    "deepseek-r1",
    "qwen-2.5-72b-instruct"
    ]
----
. Confirm the name and namespace of your `Account` CR: 
+
[source, console]
----
oc get account -A
----
+
You see output similar to the following example:
+
[source, console]
----
NAMESPACE         NAME       TEMPLATE  CONFIGMAP  SECRET
redhat-ods-applications  odh-nim-account
----
. Deploy the `ConfigMap` object in the same namespace as your `Account` CR. 
+
[source, bash]
----
oc apply -f <configmap-name> -n <namespace>
----
+ 
Replace _<configmap-name>_ with the name of your YAML file, and _<namespace>_ with the namespace of your `Account` CR.
. Add the `ConfigMap` object that you previously created to the `spec.modelListConfig` section of your `Account` CR.
+
[source, console]
----
oc patch account <account-name> \
  --type='merge' \
  	-p '{"spec": {"modelListConfig": {"name": "<configmap-name>"}}}'
----
+
Replace _<account-name>_ with the name of your `Account` CR, and _<configmap-name>_ with your `ConfigMap` object.
. Confirm that the `ConfigMap` object is added to your `Account` CR.
+
[source, console]
----
oc get account <account-name> -o yaml
----
+
You see the `ConfigMap` object in the `spec.modelListConfig` section of your `Account` CR, similar to the following output. 
+
[source, yaml]
----
spec:
 enabledModelsConfig:
 modelListConfig:
  name: <configmap-name>
----

.Verification

ifndef::upstream[]
* Follow the steps to deploy a model as described in link:{rhoaidocshome}{default-format-url}/serving_models/serving-large-models_serving-large-models#deploying-models-on-the-NVIDIA-NIM-model-serving-platform_serving-large-models[Deploying models on the NVIDIA NIM model serving platform to deploy a NIM model]. You see the models that you defined in the `ConfigMap` object in the *NVIDIA NIM* list on the *Deploy model* dialog.
endif::[]
ifdef::upstream[]
* Follow the steps to deploy a model as described in link:{odhdocshome}/serving-models/#deploying-models-on-the-NVIDIA-NIM-model-serving-platform_serving-large-models[Deploying models on the NVIDIA NIM model serving platform to deploy a NIM model]. You see the models that you defined in the `ConfigMap` object in the *NVIDIA NIM* list on the *Deploy model* dialog.
endif::[]

// [role="_additional-resources"]
// .Additional resources
