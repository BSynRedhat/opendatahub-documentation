:_module-type: PROCEDURE

[id="deploying-a-llamastackdistribution-instance_{context}"]
= Deploying a LlamaStackDistribution instance

[role='_abstract']
You can integrate LlamaStack and its retrieval-augmented generation (RAG) capabilities with your deployed Llama 3.2 model served by vLLM. This integration enables you to build intelligent applications that combine large language models (LLMs) with real-time data retrieval, providing more accurate and contextually relevant responses for your AI workloads.

When you create a `LlamaStackDistribution` custom resource (CR), specify the Llama Stack image `quay.io/opendatahub/llama-stack:odh` in the `spec.server.distribution.image` field. The image is hosted on link:https://quay.io[Quay.io], a secure registry that provides vulnerability scanning, role‑based access control, and globally distributed content delivery. Using this {org-name}–validated image ensures that your deployment automatically receives the latest security patches and compatibility updates. For more information about working with Quay.io, see link:https://docs.redhat.com/en/documentation/red_hat_quay/3/html/about_quay_io/quayio-overview[Quay.io overview].

ifdef::self-managed[]
ifdef::disconnected[]
If your cluster cannot pull images directly from public registries, first mirror the image to your local registry. For more information, see link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/disconnected_environments/mirroring-in-disconnected-environments#mirroring-images-disconnected-install[Mirroring images for disconnected installation] in the OpenShift documentation.
endif::[]
endif::[]

.Prerequisites
ifndef::upstream[]
* You have enabled GPU support in {productname-short}. This includes installing the Node Feature Discovery operator and NVIDIA GPU Operators. For more information, see link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/specialized_hardware_and_driver_enablement/psap-node-feature-discovery-operator#installing-the-node-feature-discovery-operator_psap-node-feature-discovery-operator[Installing the Node Feature Discovery operator^] and link:{rhoaidocshome}{default-format-url}/managing_openshift_ai/enabling_accelerators#enabling-nvidia-gpus_managing-rhoai[Enabling NVIDIA GPUs^].
endif::[]
ifdef::upstream[]
* You have enabled GPU support. This includes installing the Node Feature Discovery and NVIDIA GPU Operators. For more information, see link:https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html[NVIDIA GPU Operator on {org-name} OpenShift Container Platform^] in the NVIDIA documentation. 
endif::[]
* You have cluster administrator privileges for your {openshift-platform} cluster.
* You have logged in to {productname-long}.
* You have activated the Llama Stack Operator in {productname-short}.
* You have deployed an inference model, for example, the llama-3.2-3b-instruct model. 
* You have access to S3-compatible object storage, URI-based repository, or OCI-compliant registry.

.Procedure

. Log in to the OpenShift web console.
. From the left-hand navigation, select *Administrator* view.
. Click the *Quick Create* (image:images/quick-create-icon.png[]) icon and then click the *Import YAML* option.
. In the *YAML* editor that appears, create a custom resource definition (CRD) similar to the following example:
+
[source,yaml]
----
apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: lsd-llama-milvus
spec:
  replicas: 1
  server:
    containerSpec:
      resources:
        requests:
          cpu: "250m"
          memory: "500Mi"
        limits:
          cpu: "2"
          memory: "12Gi"
      env:
        - name: INFERENCE_MODEL
          value: vllm
        - name: VLLM_URL
          value: http://vllm-predictor/v1
        - name: MILVUS_DB_PATH
          value: ~/.llama/milvus.db
        - name: FMS_ORCHESTRATOR_URL
          value: "http://localhost"
      name: llama-stack
      port: 8321
    distribution:
      image: quay.io/opendatahub/llama-stack:odh
    storage:
      size: "5Gi"
----

. Click *Create*.

.Verification

* In the left-hand navigation, click *Workloads* → *Pods* and then verify that the LlamaStack pod is running in the correct namespace.
* To verify that the LlamaStack server is running, click the pod name and select the *Logs* tab. Look for output similar to the following:
+
[source,log]
----
INFO     2025-05-15 11:23:52,750 __main__:498 server: Listening on ['::', '0.0.0.0']:8321
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO     2025-05-15 11:23:52,765 __main__:151 server: Starting up
INFO:     Application startup complete.
INFO:     Uvicorn running on http://['::', '0.0.0.0']:8321 (Press CTRL+C to quit)
----
* Confirm that a Service resource for the LlamaStack backend is present in your namespace and points to the running pod. You can check this by clicking *Networking* → *Services* in the web console.