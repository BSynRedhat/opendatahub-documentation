:_module-type: REFERENCE

[id='glossary-of-common-terms-for-{productname-short}_{context}']
= Glossary of common terms for {productname-short}

This glossary defines common terms for {productname-long}.

accelerator:: In high-performance computing, a specialized circuit that is used to take some of the computational load from the CPU, increasing the efficiency of the system. For example, in deep learning, GPU-accelerated computing is often employed to offload part of the compute workload to a GPU while the main application runs off the CPU 
//Reference: IBM glossary https://dataplatform.cloud.ibm.com/docs/content/wsj/wscommon/glossary-wx.html?context=wx#x2048370

artificial intelligence (AI):: The capability to acquire, process, create and apply knowledge in the form of a model to make predictions, recommendations or decisions.
//Reference: https://dataplatform.cloud.ibm.com/docs/content/wsj/wscommon/glossary-wx.html?context=wx#x3448902

bias detection:: The process of calculating fairness metrics to detect when AI models are delivering unfair outcomes based on certain attributes.
//Reference:: IBM glossary https://dataplatform.cloud.ibm.com/docs/content/wsj/wscommon/glossary-wx.html?context=wx#x9721361

cluster storage:: A persistent volume that retains the files and data within a workbench. A workbench has access to one or more cluster storage instances.

custom resource (CR):: A resource implemented through the Kubernetes CustomResourceDefinition API. A custom resource is distinct from the built-in Kubernetes resources, such as the pod and service resources. Every CR is part of an API group.
//Reference: https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/term_glossary.adoc#custom-resource

custom resource definition (CRD):: In Red Hat OpenShift, a custom resource definition (CRD) defines a new, unique object `Kind` in the cluster and lets the Kubernetes API server handle its entire lifecycle.
// Reference:: https://redhat-documentation.github.io/supplementary-style-guide/#custom-resource-definition

data connection:: The configuration parameters required to connect to a data source, such as an S3 object bucket.

data science pipelines:: A portable machine learning (ML) workflow to develop and deploy data science models.

data science project:: An isolated environment for developing, training and serving machine learning models.

disconnected environment:: An environment on a restricted network that does not have an active connection to the Internet.
// Reference: https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/term_glossary.adoc#disconnected

distributed workloads:: Data science workloads that are run simultaneously across multiple nodes in an OpenShift cluster.

fine-tuning:: The process of adapting a pre-trained model to perform a specific task by conducting additional training. Fine tuning may involve (1) updating the model’s existing parameters, known as full fine tuning, or (2) updating a subset of the model’s existing parameters or adding new parameters to the model and training them while freezing the model’s existing parameters, known as parameter-efficient fine tuning.
// Reference: RHEL AI glossary, https://dataplatform.cloud.ibm.com/docs/content/wsj/wscommon/glossary-wx.html?context=wx#x9094307

graphics processing unit (GPU):: A specialized processor designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display. GPUs are heavily utilized in machine learning due to their parallel processing capabilities.
//Reference IBM Glossary https://dataplatform.cloud.ibm.com/docs/content/wsj/wscommon/glossary-wx.html?context=wx#x8987320

inference:: The process of using a trained AI model to generate predictions or conclusions based on the input data provided to the model. 

inference server:: A server that helps an AI model make new conclusions based on its prior training. Inference servers feed the input requests through a machine learning model and return an output.

large language model (LLM):: A language model with a large number of parameters, trained on a large quantity of text.
//Reference: https://dataplatform.cloud.ibm.com/docs/content/wsj/wscommon/glossary-wx.html?context=wx#x10298052

machine learning (ML):: A branch of artificial intelligence (AI) and computer science that focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving the accuracy of AI models.

model:: In a machine learning context, a set of functions and algorithms that have been trained and tested on a data set to provide predictions or decisions. 
// https://dataplatform.cloud.ibm.com/docs/content/wsj/wscommon/glossary-wx.html?context=wx#x2245601

model registry:: A central repository containing metadata related to machine learning models from inception to deployment. The metadata ranges from high-level information like the deployment environment and project origins, to intricate details like training hyperparameters, performance metrics, and deployment events.
// Reference: https://dataplatform.cloud.ibm.com/docs/content/wsj/wscommon/glossary-wx.html?context=wx#x8397498

MLOps:: The practice for collaboration between data scientists and operations professionals to help manage production machine learning (or deep learning) lifecycle. MLOps looks to increase automation and improve the quality of production ML while also focusing on business and regulatory requirements. It involves model development, training, validation, deployment, monitoring, and management and uses methods like CI/CD.
//Reference: https://dataplatform.cloud.ibm.com/docs/content/wsj/wscommon/glossary-wx.html?context=wx#x10072886

notebook:: An interactive document that contains executable code, descriptive text for that code, and the results of any code that is run.
// Reference https://dataplatform.cloud.ibm.com/docs/content/wsj/wscommon/glossary-wx.html?context=wx#x2031718

object storage:: A method of storing data, typically used in the cloud, in which data is stored as discrete units, or objects, in a storage pool or repository that does not use a file hierarchy but that stores all objects at the same level.
// Reference https://dataplatform.cloud.ibm.com/docs/content/wsj/wscommon/glossary-wx.html?context=wx#x5852343

OpenShift Container Platform cluster::  A group of physical machines that contains the controllers, pods, services, and configurations required to build and run containerized applications.
//Reference: https://redhat-documentation.github.io/supplementary-style-guide/#ocp-cluster

persistent volume clain (PVC):: A persistent volume claim (PVC) is a request for storage in the cluster by a user.
//Reference https://redhat-documentation.github.io/supplementary-style-guide/#persistent-volume-claim

quantization:: A method of compressing foundation model weights to speed up inferencing and reduce GPU memory needs.

serving:: The process of deploying a trained ML model into a production environment and making it available for inferencing as a network service. Real-world applications can access the deployed model by using a REST or gRPC API. 
// Reference: https://www.hopsworks.ai/dictionary/model-serving

serving runtime:: A configurable environment or system for deploying and managing a trained ML model. It also dynamically loads and unloads models of various formats and exposes a service endpoint for inferencing requests. 

vLLM:: A high-throughput and efficient inference engine for running large-language models that integrates with popular models and frameworks.

workbench:: An isolated environment for development and experimentation with ML models. Workbenches typically contain IDEs, such as JupyterLab, RStudio, and Visual Studio Code.

workbench image:: An image optimized with the tools and libraries that you need for model development. Includes an integrated development environment (IDE) for developing your machine learning (ML) models.

YAML:: A human-readable data-serialization language. It is commonly used for configuration files and in applications where data is being stored or transmitted.
