:_module-type: CONCEPT

[id="guidelines-for-metrics-based-autoscaling_{context}"]

= Guidelines for metrics-based autoscaling
[role="_abstract"]

You can use metrics-based autoscaling to scale your AI workloads based on latency or throughput-focused Service Level Objectives (SLOs) as opposed to traditional request concurrency. Metrics-based autoscaling is based on Kubernetes Event-driven Autoscaling (KEDA).

Traditional scaling methods, which depend on factors such as request concurrency, request rate, or CPU utilization, are not effective for scaling LLM inference servers that operate on GPUs. In contrast, vLLM capacity is determined by the size of the GPU and the total number of tokens processed simultaneously. You can use custom metrics to help with autoscaling decisions to meet your SLOs.

The following guidelines can help you autoscale AI inference workloads, including selecting metrics, defining sliding windows, configuring HPA scale-down settings, and taking model size into account for optimal scaling performance.

== Choosing metrics for latency and throughput-optimized scaling

For latency-sensitive applications, choose scaling metrics depending on the characteristics of the requests:

* When sequence lengths vary, use service level objectives (SLOs) for Time to First Token (TTFT) and Inter-Token Latency (ITL). These metrics provide more scaling signals because they are less affected by changes in sequence length.

* Use `end-to-end request latency` to trigger autoscaling when requests have similar sequence lengths.

End-to-end (e2e) request latency depends on sequence length, posing challenges for use cases with high variance in input/output token counts. A 10 token completion and a 2000 token completion will have vastly different latencies even under identical system conditions. To maximize throughput without latency constraints, use the `vllm:num_requests_waiting > 0.1` metric (KEDA `scaledObject` does not support a threshold of 0) to scale your workloads. This metric scales up the system as soon as a request is queued, which maximizes utilization and prevents a backlog. This strategy works best when input and output sequence lengths are consistent.

To build effective metrics-based autoscaling, follow these best practices:

* Select the right metrics:
** Analyze your load patterns to determine sequence length variance.
** Choose TTFT/ITL for high-variance workloads, and E2E latency for uniform workloads.
** Implement multiple metrics with different priorities for robust scaling decisions.

* Progressively tune configurations:
** Start with conservative thresholds and longer windows.
** Monitor scaling behavior and SLO compliance over time.
** Optimize the configuration based on observed patterns and business needs.

* Validate behavior through testing:
** Run load tests with realistic sequence length distributions.
** Validate scaling under various traffic patterns.
** Test edge cases, such as traffic spikes and gradual load increases.


== Choosing the right sliding window

The sliding window length is the time period over which metrics are aggregated or evaluated to make scaling decisions. The length of the sliding window length affects scaling responsiveness and stability.

The ideal window length depends on the metric you use:

* For Time to First Token (TTFT) and Inter-Token Latency (ITL) metrics, you can use shorter windows (1-2 minutes) because they are less noisy.
* For end-to-end latency metrics, you need longer windows (4-5 minutes) to account for variations in sequence length.

[cols="1,1,1", options="header"]  
|=== 
| Window length | Characteristics | Best for 
| Short (Less than 30 seconds) 
| Does not effectively trigger autoscaling if the metric scraping interval is too long. 
| Not recommended. 
| Medium (60 seconds) 
| Responds quickly to load changes, but may lead to higher costs. Can cause rapid scaling up and down, also known as thrashing. 
| Workloads with sharp, unpredictable spikes. 
| Long (Over 4 minutes) 
| Balances responsiveness and stability while reducing unnecessary scaling. Might miss brief spikes and adapt slowly to load changes. 
| Production workloads with moderate variability. 
|===


== Optimizing HPA scale-down configuration

Effective scale-down configuration is crucial for cost optimization and resource efficiency. It requires balancing the need to quickly terminate idle pods to reduce cluster load, with the consideration of maintaining them to avoid cold startup times. The Horizontal Pod Autoscaler (HPA) configuration for scale-down plays a critical role in removing idle pods promptly and preventing unnecessary resource usage.

You can control the HPA scale-down behavior by managing the KEDA `scaledObject` custom resource (CR). This Custom Resource (CR) enables event-driven autoscaling for a specific workload.

To set the time that the HPA waits before scaling down, adjust the `stabilizationWindowSeconds` field as shown in the following example:

[source, YAML]
----
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: my-app-scaler
spec:
  advanced:
    horizontalPodAutoscalerConfig:
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 300

----

== Considering model size for optimal scaling

Model size affects autoscaling behavior and resource use. The following table describes the typical characteristics for different model sizes and describes a scaling strategy to select when implementing metrics-based autoscaling for AI inference workloads.

[cols="1,1,1,1", options="header"]  
|=== 
| Model size | Memory footprint | Scaling strategy | Cold start time
| Small (Less than 3B)
| Less than 6 GiB
| Use aggressive scaling with lower resource buffers.
| Up to 10 minutes to download and 30 seconds to load.
| Medium (3B-10B)
| 6-20 GiB	
| Use a more conservative scaling strategy.	
| Up to 30 minutes to download and 1 minute to load.
| Large (Greater than 10B)
| Greater than 20 GiB
| May require model sharding or quantization.
| Up to several hours to download and minutes to load.
|===

For models with fewer than 3 billion parameters, you can reduce cold start latency with the following strategies:

* Optimize container images by embedding models directly into the image instead of downloading them at runtime. You can also use multi-stage builds to reduce the final image size and use image layer caching for faster container pulls.
* Cache models on a Persistent Volume Claim (PVC) to share storage across replicas. Configure your inference service to use the PVC to access the cached model.

[role="_additional-resources"]
.Additional resources
* https://docs.vllm.ai/en/latest/serving/distributed_serving.html[Distributed serving]
