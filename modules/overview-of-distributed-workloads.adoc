:_module-type: CONCEPT

[id='overview-of-distributed-workloads_{context}']
= Overview of distributed workloads

[role='_abstract']
You can use the distributed workloads feature to queue, scale, and manage the resources required to run data science workloads across multiple nodes in an OpenShift cluster simultaneously. Data science workloads include several types of artificial intelligence (AI) workloads, including machine learning (ML) and Python workloads.

Distributed workloads provide the following benefits:

* You can iterate faster and experiment more frequently because of the reduced processing time.
* You can use larger datasets, which can lead to more accurate models.
* You can use complex models that could not be trained on a single node.

The distributed workloads infrastructure includes the following components:

* *CodeFlare SDK:* Defines and controls the remote distributed compute jobs and infrastructure for any Python-based environment

* *KubeRay:* Manages remote Ray clusters on OpenShift for running distributed compute workloads

* *CodeFlare Operator:* Manages the queuing of batch jobs

You can run distributed data science workloads from data science pipelines, from Python code, or from notebooks.




////
[role="_additional-resources"]
.Additional resources
* link:https://url/[link text]
////
