:_module-type: CONCEPT

[id="overview-of-llama-stack_{context}"]
= Overview of Llama Stack

[role="_abstract"]
Llama Stack is a unified AI runtime environment designed to simplify the deployment and management of generative AI workloads on {productname-short}. Llama Stack integrates LLM inference servers, vector databases, and retrieval services in a single stack, optimized for Retrieval-Augmented Generation (RAG) and agent-based AI workflows. In {openshift-platform}, the Llama Stack Operator manages the deployment lifecycle of these components, ensuring scalability, consistency, and integration with {productname-short} projects.

ifndef::upstream[]
[IMPORTANT]
====
ifdef::self-managed[]
Llama Stack integration is currently available in {productname-long} {vernum} as a Technology Preview feature.
endif::[]
ifdef::cloud-service[]
Llama Stack integration is currently available in {productname-long} as a Technology Preview feature.
endif::[]
Technology Preview features are not supported with {org-name} production service level agreements (SLAs) and might not be functionally complete.
{org-name} does not recommend using them in production.
These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of {org-name} Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].
====
endif::[]

Llama Stack includes the following components:

* **Inference model servers** such as vLLM, designed to efficiently serve large language models.
* **Vector storage** solutions, primarily Milvus, to store embeddings generated from your domain data.
* **Retrieval and embedding management** workflows using integrated tools, such as Docling, to handle continuous data ingestion and synchronization.
* **Integration with {productname-short}** by using the `LlamaStackDistribution` custom resource, simplifying configuration and deployment.

ifdef::upstream[]
For information about how to deploy Llama Stack in {productname-short}, see link:{odhdocshome}/working-with-rag/#deploying-a-rag-stack-in-a-data-science-project_rag[Deploying a RAG stack in a Data Science Project].
endif::[]
ifndef::upstream[]
For information about how to deploy Llama Stack in {productname-short}, see link:{rhoaidocshome}{default-format-url}/working_with_rag/deploying-a-rag-stack-in-a-data-science-project_rag[Deploying a RAG stack in a Data Science Project].
endif::[]

== The `LlamaStackDistribution` custom resource API providers

The `LlamaStackDistribution` custom resource includes various API types and providers that you can use in {productname-short}. The following table displays the supported providers that are included in the distribution:

[cols="1,1,1", options="header"]
|===
|*API type* |*Providers* |*Support Status*
|Agents 
|`inline::meta-reference` 
|Developer Preview

|Datasetio 
|
`inline::localfs` +
`remote::huggingface` 
|Developer Preview

|Eval 
|`remote::lmeval` 
|Developer Preview

|Inference 
|
`inline::sentence-transformers` +
`remote::vllm` 
|Developer Preview

|Safety 
|`remote::trustyai_fms` 
|Developer Preview 

|Scoring 
|
`inline::llm-as-a-judge` +
`inline:basic` +
`inline::braintrust` 
|Developer Preview

|Telemetry 
|`inline::meta-reference` 
|Developer Preview

|Tool_runtime 
|
`inline::rag-runtime` +
`remote::brave-search` +
`remote::tavily-search` +
`remote::model-context-protocol` 
|Developer Preview

|Vector_io 
|`inline::milvus` 
|Developer Preview
|===


[role="_additional-resources"]
.Additional resources
* link:https://github.com/opendatahub-io/llama-stack-demos[Llama Stack Demos repository^]
* link:https://llama-stack-k8s-operator.pages.dev/[Llama Stack Kubernetes Operator documentation^]
* link:https://llama-stack.readthedocs.io/en/latest/[Llama Stack documentation]
