:_module-type: CONCEPT

[id="overview-of-rag_{context}"]
= Overview of RAG

Retrieval-augmented generation (RAG) in {productname-short} extends the capabilities of large language models (LLMs) by integrating domain-specific data sources directly into the modelâ€™s context. By combining retrieval from these sources with real-time generation, data scientists and AI engineers obtain tailored, accurate, and verifiable answers to complex queries based on their own datasets within a data science project. 

When AI engineers or data scientists use RAG in {productname-short}, they can gain insights from structured data, such as relational database tables, or from unstructured data, such as PDF documents, and then ingest that information to generate verifiable responses.

RAG indexes content and builds an embedding store that data scientists and AI engineers can query. When data scientists or AI engineers pose a question to a RAG chatbot, the RAG pipeline retrieves the most relevant pieces of data, passes them to the LLM as context, and generates a response that reflects both the prompt and the retrieved content.
