:_module-type: CONCEPT

[id="performance-considerations-for-document-based-apps_{context}"]

= Performance considerations for text-summarization and retrieval-augmented generation (RAG) applications
[role="_abstract"]

There are additional factors that need to be taken into consideration for text-summarization and RAG applications, as well as for LLM-powered services that process large documents uploaded by users.

* *Longer Input Sequences*: The input sequence length can be significantly longer than in a typical chat application, if each user query includes a large prompt or a large amount of context such as an uploaded document. The longer input sequence length increases the **prefill time**, the time the model takes to process the initial input sequence before generating a response, which can then lead to a higher **Time-to-First-Token (TTFT)**. A longer TTFT may impact the responsiveness of the application. Minimize this latency for optimal user experience.

* *KV Cache Usage*: Longer sequences require more GPU memory for the **key-value (KV) cache**. The KV cache stores intermediate attention data to improve model performance during generation. A high KV cache utilization per request requires a hardware setup with sufficient GPU memory. This is particularly crucial if multiple users are querying the model concurrently, as each request adds to the total memory load.

* *Optimal Hardware Configuration*: To maintain responsiveness and avoid memory bottlenecks, select a GPU configuration with sufficient memory. For instance, instead of running an 8B model on a single 24GB GPU, deploying it on a larger GPU (e.g., 48GB or 80GB) or across multiple GPUs can improve performance by providing more memory headroom for the KV cache and reducing inter-token latency. Multi-GPU setups with tensor parallelism can also help manage memory demands and improve efficiency for larger input sequences.

In summary, to ensure optimal responsiveness and scalability for document-based applications, you must prioritize hardware with high GPU memory capacity and also consider multi-GPU configurations to handle the increased memory requirements of long input sequences and KV caching.

//[role="_additional-resources"]
//.Additional resources
