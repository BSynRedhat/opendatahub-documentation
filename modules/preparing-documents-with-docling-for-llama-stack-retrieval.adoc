:_module-type: PROCEDURE

[id="preparing-documents-with-docling-for-llama-stack-retrieval_{context}"]
= Preparing documents with Docling for Llama Stack retrieval

[role="_abstract"]
You can transform your source documents with a Docling-enabled data science pipeline and ingest the output into a Llama Stack vector store by using the Llama Stack SDK. This modular approach separates document preparation from ingestion, yet still delivers an end-to-end, retrieval-augmented generation (RAG) workflow.

The pipeline registers a Milvus vector database and downloads the source PDFs, then splits them for parallel processing and converts each batch to Markdown with Docling. It generates sentence-transformer embeddings from the Markdown and stores them in the vector store, making the documents instantly searchable in Llama Stack.

.Prerequisites
* You have logged in to {openshift-platform} web console.
* You have a data science project and access to Pipelines in the {productname-short} dashboard.
* You have created and configured a pipeline server within the data science project that contains your workbench.
* You have configured a Llama Stack deployment.
* You have deployed a Llama 3.2 model with a vLLM model server and you are aware of its endpoint URL.
* You have created a project workbench within a data science project.
* You have opened a Jupyter notebook and it is running in your workbench environment.
* You have a created and configured a vector database instance and you know its identifier. Alternatively, you know the credentials required to register a vector database instance. 
ifdef::self-managed[]
* Your environment has network access to the vector database service through {openshift-platform}.
endif::[]
* You have installed local object storage buckets and created connections, as described in xref:storing-data-with-connections[Storing data with connections].
ifndef::upstream[]
* You have enabled GPU support in {productname-short}. This includes installing the Node Feature Discovery operator and NVIDIA GPU Operators. For more information, see link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/specialized_hardware_and_driver_enablement/psap-node-feature-discovery-operator#installing-the-node-feature-discovery-operator_psap-node-feature-discovery-operator[Installing the Node Feature Discovery operator^] and link:{rhoaidocshome}{default-format-url}/managing_openshift_ai/enabling_accelerators#enabling-nvidia-gpus_managing-rhoai[Enabling NVIDIA GPUs^].
endif::[]
ifdef::upstream[]
* You have enabled GPU support. This includes installing the Node Feature Discovery and NVIDIA GPU Operators. For more information, see link:https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html[NVIDIA GPU Operator on {org-name} OpenShift Container Platform^] in the NVIDIA documentation. 
endif::[]
* You have a pipeline that contains a Docling transform and you have compiled this pipeline into YAML format. 
* You have PDF documents ready for ingestion and know their storage location.
* Your data science project quota allows between 500 millicores (0.5 CPU) and 4 CPU cores for the pipeline run.
* Your data science project quota allows from 2 GiB up to 6 GiB of RAM for the pipeline run.
* If you are using GPU acceleration, you have at least one NVIDIA GPU available.

.Procedure
. In a new notebook cell, install the `llama_stack` client package:
+
[source,python]
----
%pip install llama_stack
----

. In a new notebook cell, import Agent, AgentEventLogger, and LlamaStackClient:
+
[source,python]
----
from llama_stack_client import Agent, AgentEventLogger, LlamaStackClient
----

. In a new notebook cell, assign your deployment endpoint to the `base_url` parameter to create a LlamaStackClient instance:
+
[source,python]
----
client = LlamaStackClient(base_url="<your deployment endpoint>")
----

. List the available models:
+
[source,python]
----
models = client.models.list()
----

. Select the first LLM and the first embedding model:
+
[source,python]
----
model_id = next(m.identifier for m in models if m.model_type == "llm")
embedding_model = next(m for m in models if m.model_type == "embedding")
embedding_model_id = embedding_model.identifier
embedding_dimension = embedding_model.metadata["embedding_dimension"]
----

. In a new notebook cell, define the following parameters:
* `vector_db_id`: a unique name that identifies your vector database, for example, `my_milvus_db`.  
* `provider_id`: the connector key that your Llama Stack gateway has enabled. For the Milvus vector database, this connector key is `"milvus"`. You can also list the available connectors:
+
[source,python]
----
print(client.vector_dbs.list_providers()) # lists available connectors

vector_db_id = "<your vector database ID>"
provider_id  = "<your provider ID>"
----

. In a new notebook cell, register or confirm your vector database to store embeddings:
+
[source,python]
----
_ = client.vector_dbs.register(
vector_db_id=vector_db_id,
embedding_model=embedding_model_id,
embedding_dimension=embedding_dimension,
provider_id=provider_id,
)
print(f"Registered vector DB: {vector_db_id}")
----

ifndef::upstream[]
. In the {openshift-platform} web console, import the YAML file containing your docling pipeline into your data science project, as described in link:{rhoaidocshome}{default-format-url}/working_with_data_science_pipelines/managing-data-science-pipelines_ds-pipelines#importing-a-data-science-pipeline_ds-pipelines[Importing a data science pipeline].
endif::[]
ifdef::upstream[]
. In the {openshift-platform} web console, import your YAML file containing your docling pipeline into your data science project, as described in link:{odhdocshome}/working-with-data-science-pipelines/#importing-a-data-science-pipeline_ds-pipelines[Importing a pipeline version].
endif::[]

ifndef::upstream[]
. Create a pipeline run to execute your Docling pipeline, as described in link:{rhoaidocshome}{default-format-url}/working_with_data_science_pipelines/managing-data-science-pipelines_ds-pipelines#executing-a-pipeline-run_ds-pipelines[Executing a pipeline run]. The pipeline run inserts your PDF documents into the vector database. Before you run the Docling pipeline, you can customize the following parameters:

* base_url: The bse URL to fetch PDF files from.
* pdf_filenames: A Comma-separated list of PDF filenames to download and convert.
* num_workers: The number of parallel workers.
* vector_db_id: The Milvus vector database ID.
* service_url: The Milvus service URL.
* embed_model_id: The embedding model to use.
* max_tokens: The maximum tokens for each chunk.
* use_gpu: Enable or disable GPU acceleration.
endif::[]

ifdef::upstream[]
. Create a pipeline run to execute your Docling pipeline, as described in link:{odhdocshome}/working-with-data-science-pipelines/#executing-a-pipeline-run_ds-pipelines[Executing a pipeline run]. The pipeline run inserts your PDF documents into the vector database. Before you run the Docling pipeline, you can customize the following parameters:

* base_url: The base URL to fetch PDF files from.
* pdf_filenames: A Comma-separated list of PDF filenames to download and convert.
* num_workers: The number of parallel workers.
* vector_db_id: The Milvus vector database ID.
* service_url: The Milvus service URL.
* embed_model_id: The embedding model to use.
* max_tokens: The maximum tokens for each chunk.
* use_gpu: Enable or disable GPU acceleration.
endif::[]

.Verification

. To ascertain if the document ingestion has been successful, in your Jupyter notebook, query the LLM with a question that relates to the ingested content. Here is an example:   
+
[source,python]
----
from llama_stack_client import Agent, AgentEventLogger
import uuid

rag_agent = Agent(
    client,
    model=model_id,
    instructions="You are a helpful assistant",
    tools=[
        {
            "name": "builtin::rag/knowledge_search",
            "args": {"vector_db_ids": [vector_db_id]},
        }
    ],
)

prompt = "What can you tell me about the birth of word processing?"
print("prompt>", prompt)

session_id = rag_agent.create_session(session_name=f"s{uuid.uuid4().hex}")

response = rag_agent.create_turn(
    messages=[{"role": "user", "content": prompt}],
    session_id=session_id,
    stream=True,
)

for log in AgentEventLogger().log(response):
    log.print()
----

. Query chunks from the vector database:
+
[source,python]
----
query_result = client.vector_io.query(
    vector_db_id=vector_db_id,
    query="what do you know about?",
)
print(query_result)
----

