:_module-type: PROCEDURE

[id="running-distributed-data-science-workloads-from-ds-pipeline_{context}"]
= Running distributed data science workloads from data science pipelines

[role='_abstract']
To run a distributed data science workload from a data science pipeline, you must first update the pipeline to include a link to your Ray cluster image.

.Prerequisites
* You have access to a data science cluster that is configured to run distributed workloads as described in link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/working-with-distributed-workloads_distributed-workloads#configuring-distributed-workloads_distributed-workloads[Configuring distributed workloads].
* You have installed the {org-name} OpenShift Pipelines Operator as described in link:https://access.redhat.com/documentation/en-us/openshift_container_platform/{ocp-latest-version}/html/cicd/pipelines#installing-pipelines[Installing OpenShift Pipelines].
* You have access to S3-compatible object storage.
* You have created a data science project.

.Procedure
. Create a data connection to connect the object storage to your data science project, as described in link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/working-on-data-science-projects_nb-server#adding-a-data-connection-to-your-data-science-project_nb-server[Adding a data connection to your data science project].
. Configure a pipeline server to use the data connection, as described in link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/working-with-data-science-pipelines_ds-pipelines#configuring-a-pipeline-server_ds-pipelines[Configuring a pipeline server].
. Create the data science pipeline as follows:
.. Build your data science pipeline in Python code.
For example, create a file named `compile_example.py` with the following content:
+
[source,Python]
----
from kfp import components, dsl


def ray_fn(openshift_server: str, openshift_token: str) -> int:
   import ray
   from codeflare_sdk.cluster.auth import TokenAuthentication
   from codeflare_sdk.cluster.cluster import Cluster, ClusterConfiguration


   auth = TokenAuthentication(
       token=openshift_token, server=openshift_server, skip_tls=True
   )
   auth_return = auth.login()
   cluster = Cluster(
       ClusterConfiguration(
           name="raytest",
           # namespace must exist
           namespace="pipeline-example",
           num_workers=1,
           head_cpus="500m",
           min_memory=1,
           max_memory=1,
           num_gpus=0,
           image="quay.io/project-codeflare/ray:latest-py39-cu118",
           instascale=False,
       )
   )


   print(cluster.status())
   cluster.up()
   cluster.wait_ready()
   print(cluster.status())
   print(cluster.details())


   ray_dashboard_uri = cluster.cluster_dashboard_uri()
   ray_cluster_uri = cluster.cluster_uri()
   print(ray_dashboard_uri, ray_cluster_uri)


   # before proceeding make sure the cluster exists and the uri is not empty
   assert ray_cluster_uri, "Ray cluster needs to be started and set before proceeding"


   ray.init(address=ray_cluster_uri)
   print("Ray cluster is up and running: ", ray.is_initialized())


   @ray.remote
   def train_fn():
       # complex training function
       return 100


   result = ray.get(train_fn.remote())
   assert 100 == result
   ray.shutdown()
   cluster.down()
   auth.logout()
   return result


@dsl.pipeline(
   name="Ray Simple Example",
   description="Ray Simple Example",
)
def ray_integration(openshift_server, openshift_token):
   ray_op = components.create_component_from_func(
       ray_fn,
       base_image='registry.redhat.io/ubi8/python-39:latest',
       packages_to_install=["codeflare-sdk"],
   )
   ray_op(openshift_server, openshift_token)


if __name__ == '__main__':
    from kfp_tekton.compiler import TektonCompiler
    TektonCompiler().compile(ray_integration, 'compiled-example.yaml')

----
.. Install any required dependencies.
In this example, install the `kfp` and `kfp-tekton` Python packages:
+
[source,bash]
----
$ pip install kfp kfp-tekton
----
.. Compile the Python file (in this example, the `compile_example.py` file):
+
[source,bash]
----
$ python compile_example.py
----
This command creates a YAML file (in this example, `compiled-example.yaml`), which you can import in the next step.
. Import your data science pipeline as described in link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/working-with-data-science-pipelines_ds-pipelines#importing-a-data-science-pipeline_ds-pipelines[Importing a data science pipeline].
. Schedule the pipeline run as described in link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/working-with-data-science-pipelines_ds-pipelines#managing_pipeline_runs[Scheduling a pipeline run].
. When the pipeline run is complete, view the run details as described in link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/working-with-data-science-pipelines_ds-pipelines#viewing-the-details-of-a-pipeline-run_ds-pipelines[Viewing the details of a pipeline run].

.Verification
The YAML file is created and the pipeline run completes without errors.

////
[role='_additional-resources']
.Additional resources
<Do we want to link to additional resources?>


* link:https://url[link text]
////
