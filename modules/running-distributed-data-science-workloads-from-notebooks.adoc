:_module-type: PROCEDURE

[id="running-distributed-data-science-workloads-from-notebooks_{context}"]
= Running distributed data science workloads from notebooks

[role='_abstract']
To run a distributed workload from a notebook, you must configure a Ray cluster.
You must also provide environment-specific information such as cluster authentication details.

In the examples in this procedure, you edit the demo notebooks to provide the required information.

.Prerequisites
ifndef::upstream[]
* You can access a data science cluster that is configured to run distributed workloads as described in link:{rhoaidocshome}{default-format-url}/working_with_distributed_workloads/configuring-distributed-workloads_distributed-workloads[Configuring distributed workloads].
endif::[]
ifdef::upstream[]
* You can access a data science cluster that is configured to run distributed workloads as described in link:{odhdocshome}/working-with-distributed-workloads/#configuring-distributed-workloads_distributed-workloads[Configuring distributed workloads].
endif::[]

ifndef::upstream[]
* Your cluster administrator has created the required Kueue resources as described in link:{rhoaidocshome}{default-format-url}/working_with_distributed_workloads/configuring-distributed-workloads_distributed-workloads#configuring-quota-management-for-distributed-workloads_distributed-workloads[Configuring quota management for distributed workloads].
endif::[]
ifdef::upstream[]
* Your cluster administrator has created the required Kueue resources as described in link:{odhdocshome}/working-with-distributed-workloads/#configuring-quota-management-for-distributed-workloads_distributed-workloads[Configuring quota management for distributed workloads].
endif::[]

ifndef::upstream[]
* Optional: Your cluster administrator has defined a _default_ local queue for the Ray cluster by creating a `LocalQueue` resource and adding the following annotation to the configuration details for that `LocalQueue` resource, as described in link:{rhoaidocshome}{default-format-url}/working_with_distributed_workloads/configuring-distributed-workloads_distributed-workloads#configuring-quota-management-for-distributed-workloads_distributed-workloads[Configuring quota management for distributed workloads]:
+
[source,bash]
----
"kueue.x-k8s.io/default-queue": "true"
----
+
[NOTE]
====
If your cluster administrator does not define a default local queue, you must specify a local queue in each notebook.
====
endif::[]
ifdef::upstream[]
* Optional: Your cluster administrator has defined a _default_ local queue for the Ray cluster by creating a `LocalQueue` resource and adding the following annotation to the configuration details for that `LocalQueue` resource, as described in link:{odhdocshome}/working-with-distributed-workloads/#configuring-quota-management-for-distributed-workloads_distributed-workloads[Configuring quota management for distributed workloads]:
+
[source,bash]
----
"kueue.x-k8s.io/default-queue": "true"
----
+
[NOTE]
====
If your cluster administrator does not define a default local queue, you must specify a local queue in each notebook.
====
endif::[]

* You can access the following software from your data science cluster:
** A Ray cluster image that is compatible with your hardware architecture
** The data sets and models to be used by the workload
** The Python dependencies for the workload, either in a Ray image or in your own Python Package Index (PyPI) server

ifndef::upstream[]
* You can access a data science project that contains a workbench, and the workbench is running a default notebook image that contains the CodeFlare SDK, for example, the *Standard Data Science* notebook. 
For information about projects and workbenches, see link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects[Working on data science projects].
endif::[]
ifdef::upstream[]
* You can access a data science project that contains a workbench, and the workbench is running a default notebook image that contains the CodeFlare SDK, for example, the *Standard Data Science* notebook. 
For information about projects and workbenches, see link:{odhdocshome}/working-on-data-science-projects[Working on data science projects].
endif::[]

* You have Admin access for the data science project.
** If you created the project, you automatically have Admin access. 
** If you did not create the project, your cluster administrator must give you Admin access.

* You have logged in to {productname-long}.
* You have launched your notebook server and logged in to your notebook editor.
The examples in this procedure refer to the JupyterLab integrated development environment (IDE).

ifndef::upstream[]
* You have downloaded the demo notebooks provided by the CodeFlare SDK, as described in link:{rhoaidocshome}{default-format-url}/working_with_distributed_workloads/running-distributed-workloads_distributed-workloads#downloading-the-demo-notebooks-from-the-codeflare-sdk_distributed-workloads[Downloading the demo notebooks from the CodeFlare SDK].
endif::[]
ifdef::upstream[]
* You have downloaded the demo notebooks provided by the CodeFlare SDK, as described in link:{odhdocshome}/working_with_distributed_workloads/#downloading-the-demo-notebooks-from-the-codeflare-sdk_distributed-workloads[Downloading the demo notebooks from the CodeFlare SDK].
endif::[]


.Procedure
. In the JupyterLab interface, open the *demo-notebooks > guided-demos* folder. 
. Double-click each notebook in turn to open the notebook.
Notebook files are named `_<filename>_.ipynb`.
. In each notebook, ensure that the `import` section imports the required components from the CodeFlare SDK, as follows:
+
.Example import section
[source,bash]
----
from codeflare_sdk import Cluster, ClusterConfiguration, TokenAuthentication
----

. In each notebook, update the `TokenAuthentication` section to provide the `token` and `server` details to authenticate to the OpenShift cluster by using the CodeFlare SDK.
+
You can identify your token and server details as follows:

.. In the {productname-short} top navigation bar, click the application launcher icon (image:images/osd-app-launcher.png[The application launcher]) and then click *OpenShift Console* to open the OpenShift web console.
.. In the upper-right corner of the OpenShift web console, click your user name and select *Copy login command*. 
.. After you have logged in, click *Display Token*.
.. In the *Log in with this token* section, find the required values as follows:
* The `token` value is the text after the `--token=` prefix.
* The `server` value is the text after the `--server=` prefix.

+
[NOTE]
====
The `token` and `server` values are security credentials, treat them with care.

* Do not save the token and server details in a notebook. 
* Do not store the token and server details in Git.

The token expires after 24 hours.
====

. If you want to use custom certificates, update the `TokenAuthentication` section to add the `ca_cert_path` parameter to specify the location of the custom certificates, as shown in the following example:
+
.Example authentication section
[source,bash]
----
auth = TokenAuthentication(
    token = "XXXXX",
    server = "XXXXX",
    skip_tls=False,
    ca_cert_path="/path/to/cert"
)
auth.login()
----
+
Alternatively, you can set the `CF_SDK_CA_CERT_PATH` environment variable to specify the location of the custom certificates.

. In each notebook, update the cluster configuration section as follows:
.. If the `namespace` value is specified, replace the example value with the name of your project.
If you omit this line, the Ray cluster is created in the current project. 
.. If the `image` value is specified, replace the example value with a link to your Ray cluster image.
If you omit this line, the default community Ray image `quay.io/rhoai/ray:2.23.0-py39-cu121` is used.
+
[NOTE]
====
The default Ray image is an AMD64 image, which might not work on other architectures.
====

ifndef::upstream[]
.. If you use Kueue to manage resource quotas and your cluster administrator has not configured a default local queue as described in link:{rhoaidocshome}{default-format-url}/working_with_distributed_workloads/configuring-distributed-workloads_distributed-workloads#configuring-quota-management-for-distributed-workloads_distributed-workloads[Configuring quota management for distributed workloads], specify the local queue for the Ray cluster, as shown in the following example:
+
.Example local queue assignment
[source,bash,subs="+quotes"]
----
local_queue="_your_local_queue_name_"
----
+
You can show the list of local queues for your project as follows:

... In the OpenShift web console, select your project from the *Project* list.
... Click *Search*, and from the *Resources* list, select *LocalQueue*.
endif::[]
ifdef::upstream[]
.. If you use Kueue to manage resource quotas and your cluster administrator has not configured a default local queue as described in link:{odhdocshome}/working-with-distributed-workloads/#configuring-quota-management-for-distributed-workloads_distributed-workloads[Configuring quota management for distributed workloads], specify the local queue for the Ray cluster, as shown in the following example:
+
.Example local queue assignment
[source,bash]
----
local_queue="your_local_queue_name"
----
+
You can see the list of local queues for your project as follows:

... In the OpenShift web console, select your project from the *Project* list.
... Click *Search*, and from the *Resources* list, select *LocalQueue*.
endif::[]

.. In disconnected environments, if any of the Python packages required by the workload are not available in the Ray cluster, configure the Ray cluster to download the Python packages from a private PyPI server.
+
For example, set the `PIP_INDEX_URL` and `PIP_TRUSTED_HOST` environment variables for the Ray cluster, to specify the location of the Python dependencies, as shown in the following example:
+
----
PIP_INDEX_URL: https://pypi-notebook.apps.mylocation.com/simple
PIP_TRUSTED_HOST: pypi-notebook.apps.mylocation.com
----
where
* `PIP_INDEX_URL` specifies the base URL of your private PyPI server (the default value is https://pypi.org).
* `PIP_TRUSTED_HOST` configures Python to mark the specified host as trusted, regardless of whether that host has a valid SSL certificate or is using a secure channel.
.. Optional: Assign a dictionary of `labels` parameters to the Ray cluster for identification and management purposes, as shown in the following example:
+
.Example labels assignment
[source,bash,subs="+quotes"]
----
labels = {"exampleLabel1": "exampleLabel1Value", "exampleLabel2": "exampleLabel2Value"}
----

. In the `2_basic_interactive.ipynb` notebook, ensure that the following Ray cluster authentication code is included after the Ray cluster creation section.
+
.Ray cluster authentication code
[source,bash,subs="+quotes"]
----
from codeflare_sdk import generate_cert
generate_cert.generate_tls_cert(cluster.config.name, cluster.config.namespace)
generate_cert.export_env(cluster.config.name, cluster.config.namespace)
----
+
[NOTE]
====
Mutual Transport Layer Security (mTLS) is enabled by default in the CodeFlare component in {productname-short}.
You must include the Ray cluster authentication code to enable the Ray client that runs within a notebook to connect to a secure Ray cluster that has mTLS enabled.
====

ifndef::upstream[]
. In each notebook, run each cell in turn, and review the cell output.
+
If an error is shown, review the output to find information about the problem and the required corrective action. 
For example, replace any deprecated parameters as instructed.
See also link:{rhoaidocshome}{default-format-url}/working_with_distributed_workloads/troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads[Troubleshooting common problems with distributed workloads for users].
endif::[]
ifdef::upstream[]
. In each notebook, run each cell in turn, and review the cell output.
+
If an error is shown, review the output to find information about the problem and the required corrective action. 
For example, replace any deprecated parameters as instructed.
See also link:{odhdocshome}/working_with_distributed_workloads/#troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads[Troubleshooting common problems with distributed workloads for users].
endif::[]

.Verification
The notebooks run to completion without errors. In the notebooks, the output from the `cluster.status()` function or `cluster.details()` function indicates that the Ray cluster is `Active`.

////
[role='_additional-resources']
.Additional resources
<Do we want to link to additional resources?>


* link:https://url[link text]
////
