:_module-type: PROCEDURE

[id="running-the-lora-training-job_{context}"]
= Running the LoRA training job

[role='_abstract']
You can run a training job to tune a model with LoRA. 
The example training job in this section is based on the IBM and Hugging Face tuning example provided link:https://github.com/foundation-model-stack/fms-hf-tuning/tree/main/examples/prompt_tuning_twitter_complaints[here]. 


.Prerequisites
ifdef::upstream,self-managed[]
* You have logged in to {openshift-platform} with the `cluster-admin` role.
endif::[]
ifdef::cloud-service[]
* You have logged in to OpenShift with the `cluster-admin` role.
endif::[]

ifndef::upstream[]
* You have access to a data science cluster that is configured to run distributed workloads as described in link:{rhoaidocshome}{default-format-url}/working_with_distributed_workloads/configuring-distributed-workloads_distributed-workloads[Configuring distributed workloads].
endif::[]
ifdef::upstream[]
* You have access to a data science cluster that is configured to run distributed workloads as described in link:{odhdocshome}/working-with-distributed-workloads/#configuring-distributed-workloads_distributed-workloads[Configuring distributed workloads].
endif::[]

ifndef::upstream[]
* You have created a data science project. 
For information about how to create a project, see link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/using-data-science-projects_projects#creating-a-data-science-project_projects[Creating a data science project].
endif::[]
ifdef::upstream[]
* You have created a data science project. 
For information about how to create a project, see link:{odhdocshome}/working-on-data-science-projects/#creating-a-data-science-project_projects[Creating a data science project].
endif::[]

* You have Admin access for the data science project.
** If you created the project, you automatically have Admin access. 
** If you did not create the project, your cluster administrator must give you Admin access.

ifndef::upstream[]
* If Kueue is not installed in your environment, you have configured the Training Operator permissions as described in link:{rhoaidocshome}{default-format-url}/working_with_distributed_workloads/tuning-a-model-by-using-the-training-operator_distributed-workloads/#configuring-the-training-operator-permissions-when-not-using-kueue_distributed-workloads[Configuring the Training Operator permissions when not using Kueue].
endif::[]
ifdef::upstream[]
* If Kueue is not installed in your environment, you have configured the Training Operator permissions as described in link:{odhdocshome}/working-with-distributed-workloads/#configuring-the-training-operator-permissions-when-not-using-kueue_distributed-workloads[Configuring the Training Operator permissions when not using Kueue].
endif::[]

* You have access to a model.
* You have access to data that you can use to train the model.

ifndef::upstream[]
* You have configured the LoRA training job as described in link:{rhoaidocshome}{default-format-url}/working_with_distributed_workloads/tuning-a-model-by-using-the-training-operator_distributed-workloads#configuring-the-lora-training-job_distributed-workloads[Configuring the LoRA training job].
endif::[]
ifdef::upstream[]
* You have configured the LoRA training job as described in link:{odhdocshome}/working-with-distributed-workloads/#configuring-the-lora-training-job_distributed-workloads[Configuring the LoRA training job].
endif::[]


.Procedure
. In a terminal window, if you are not already logged in to your OpenShift cluster as a cluster administrator, log in to the OpenShift CLI as shown in the following example:
+
[source,subs="+quotes"]
----
$ oc login __<openshift_cluster_url>__ -u __<admin_username>__ -p __<password>__
----

. Create a LoRA PyTorch training job, as follows:
.. Create a YAML file named `lora_pytorchjob.yaml`.
.. Add the following `PyTorchJob` object definition:
+
[source]
----
apiVersion: kubeflow.org/v1
kind: PyTorchJob
metadata:
  name: lora-demo
  namespace: kfto
spec:
  pytorchReplicaSpecs:
    Master:
      replicas: 1
      restartPolicy: Never
      template:
        spec:
          containers:
            - env:
                - name: SFT_TRAINER_CONFIG_JSON_PATH
                  value: /etc/config/lora_config.json
              image: 'quay.io/modh/fms-hf-tuning:release'
              imagePullPolicy: IfNotPresent
              name: pytorch
              volumeMounts:
                - mountPath: /etc/config
                  name: config-volume
                - mountPath: /data/input
                  name: dataset-volume
                - mountPath: /data/output
                  name: model-volume
          volumes:
            - configMap:
                items:
                  - key: lora_config.json
                    path: lora_config.json
                name: lora-config
              name: config-volume
            - configMap:
                name: twitter-complaints
              name: dataset-volume
            - name: model-volume
              persistentVolumeClaim:
                claimName: trained-model
  runPolicy:
    suspend: false

----
.. Replace the example namespace value `kfto` with the name of your project, and update the other parameters to suit your environment.
.. Edit the parameters of the LoRA PyTorch training job, to provide the details for your training job and environment.
.. Save your changes in the `lora_pytorchjob.yaml` file.
.. Apply the configuration to run the LoRA PyTorch training job:
+
[source]
----
$ oc apply -f lora_pytorchjob.yaml
----




.Verification
ifdef::upstream,self-managed[]
. In the {openshift-platform} console, select your project from the *Project* list. 
endif::[]
ifdef::cloud-service[]
. In the OpenShift console, select your project from the *Project* list.
endif::[]
. Click *Workloads* -> *Pods* and verify that the *_<training-job-name>_-master-0* pod is listed. 


////
[role='_additional-resources']
.Additional resources
<Do we want to link to additional resources?>


* link:https://url[link text]
////
